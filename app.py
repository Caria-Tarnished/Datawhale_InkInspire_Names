import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import random
import re
import json

def get_models():
    tokenizer = AutoTokenizer.from_pretrained('IEITYuan/Yuan2-2B-Mars-hf/', trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained('IEITYuan/Yuan2-2B-Mars-hf/', trust_remote_code=True).to('cuda')
    embedding_model = SentenceTransformer('AI-ModelScope/bge-small-zh-v1.5').to('cuda')
    document_embeddings = np.load('poetry_embeddings.npy')
    with open('poetry_corpus.json', 'r', encoding='utf-8') as f:
        corpus = json.load(f)
    return tokenizer, model, embedding_model, document_embeddings, corpus

def retrieve_relevant_documents(query, embedding_model, document_embeddings, corpus, top_k=10):
    query_embedding = embedding_model.encode([query])
    index = faiss.IndexFlatL2(document_embeddings.shape[1])
    index.add(document_embeddings)
    _, indices = index.search(query_embedding, top_k)
    
    all_poems = []
    for category in corpus.values():
        all_poems.extend(category)
    
    relevant_docs = [all_poems[i] for i in indices[0] if i < len(all_poems)]
    return relevant_docs

def extract_name_suggestions(docs):
    suggestions = []
    for doc in docs:
        if 'é€‚åˆä½œåå­—çš„å­—è¯' in doc:
            suggestions.extend(doc['é€‚åˆä½œåå­—çš„å­—è¯'].split('ã€'))
    return list(set(suggestions))

def generate_content(tokenizer, model, prompt, max_retries=2):
    for _ in range(max_retries):
        try:
            inputs = tokenizer(prompt, return_tensors="pt").to('cuda')
            outputs = model.generate(inputs["input_ids"], max_length=1024, num_return_sequences=1, temperature=0.7)
            content = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
            return content
        except Exception as e:
            print(f"Error in generate_content: {e}")
    return None

def remove_eod(text):
    return re.sub(r'<eo.*?>', '', text).strip()

def generate_names(tokenizer, model, surname, gender, characteristics, style, relevant_poems, name_suggestions):
    meaningful_words = set()
    for poem in relevant_poems:
        words = re.findall(r'[\u4e00-\u9fa5]{1,2}', poem['content'])
        meaningful_words.update(words)
    
    prompt = f"""ä¸ºå§“{surname}çš„{gender}åˆ›ä½œ2ä¸ªå¯Œæœ‰è¯—æ„çš„åå­—ã€‚åå­—åº”ä½“ç°ï¼š{characteristics}ã€‚è¯—è¯é£æ ¼ï¼š{style}ã€‚
è¦æ±‚ï¼šæ¯ä¸ªåå­—2ä¸ªå­—ï¼Œæ¥è‡ªä»¥ä¸‹å‚è€ƒå†…å®¹ã€‚

å‚è€ƒï¼š
{' '.join([poem['content'] for poem in relevant_poems[:2]])}
{', '.join(list(meaningful_words)[:5])}
{', '.join(name_suggestions[:3])}

è¯·ä»¥"å§“å1ï¼š[å]ï¼Œå§“å2ï¼š[å]"çš„æ ¼å¼ç»™å‡ºï¼š"""

    response = generate_content(tokenizer, model, prompt)
    if not response:
        return []
    names = re.findall(r'å§“å\dï¼š([\u4e00-\u9fa5]{2})', response)
    return names[:2]

def generate_meaning(tokenizer, model, surname, name, gender, characteristics, style):
    prompt = f"""è§£é‡Š{surname}{name}è¿™ä¸ª{gender}åå­—çš„å«ä¹‰ã€‚ä½“ç°ï¼š{characteristics}ã€‚é£æ ¼ï¼š{style}ã€‚
è¦æ±‚ï¼šç®€æ´ã€æœ‰åˆ›æ„ï¼Œä¸è¶…è¿‡20å­—ã€‚

è¯·ç»™å‡ºåå­—å«ä¹‰ï¼š"""

    response = generate_content(tokenizer, model, prompt)
    return remove_eod(response) if response else "æœªèƒ½ç”Ÿæˆå«ä¹‰"

def app(surname, gender, characteristics, style):
    tokenizer, model, embedding_model, document_embeddings, corpus = get_models()
    
    query = f"{gender} {characteristics} {style}"
    relevant_poems = retrieve_relevant_documents(query, embedding_model, document_embeddings, corpus)
    name_suggestions = extract_name_suggestions(relevant_poems)
    
    names = generate_names(tokenizer, model, surname, gender, characteristics, style, relevant_poems, name_suggestions)
    
    meanings = []
    for name in names:
        full_name = surname + name
        meaning = generate_meaning(tokenizer, model, surname, name, gender, characteristics, style)
        meanings.append((full_name, meaning))
    
    if names:
        referenced_poems = [
            f"{poem['title']}Â·{poem['author']}ï¼š{poem['content']}" for poem in relevant_poems[:2]
        ]
        return (
            ', '.join([surname + name for name in names]),
            meanings,
            '\n\n'.join(referenced_poems),
            ", ".join(name_suggestions[:5])
        )
    else:
        return ("åå­—ç”Ÿæˆå¤±è´¥ï¼Œè¯·é‡è¯•ã€‚", [], "", "")

iface = gr.Interface(
    fn=app,
    inputs=[
        gr.Textbox(label="è¯·è¾“å…¥å®å®çš„å§“æ°"),
        gr.Radio(["ç”·å­©", "å¥³å­©"], label="å®å®çš„æ€§åˆ«"),
        gr.Textbox(label="æ‚¨å¸Œæœ›åå­—ä½“ç°å“ªäº›å“è´¨æˆ–æ„å¢ƒï¼Ÿ"),
        gr.Textbox(label="æ‚¨åå¥½çš„è¯—è¯æ„å¢ƒ"),
    ],
    outputs=[
        gr.Textbox(label="ä¸ºæ‚¨ç”Ÿæˆçš„åå­—"),
        gr.JSON(label="åå­—å«ä¹‰"),
        gr.Textbox(label="å‚è€ƒè¯—å¥åŸæ–‡"),
        gr.Textbox(label="å»ºè®®çš„åå­—å­—è¯")
    ],
    title="ğŸ“œ æ–‡å¢¨å¯å",
    description="é€šè¿‡ã€Šè¯—ç»ã€‹ã€ã€Šæ¥šè¾ã€‹ã€ã€Šå”è¯—ä¸‰ç™¾é¦–ã€‹ã€ã€Šå®‹è¯ã€‹ç­‰å¤å…¸è¯—è¯ï¼Œä¸ºå®å®å–ä¸€ä¸ªæœ‰æ–‡åŒ–åº•è•´çš„åå­—ã€‚",
    article="""
    æ³¨ï¼šæœ¬å·¥å…·ä»…ä¾›å‚è€ƒï¼Œæœ€ç»ˆå–åè¯·ç»“åˆæ‚¨ä¸ªäººå–œå¥½å’Œæ–‡åŒ–èƒŒæ™¯ã€‚
    """
)

if __name__ == "__main__":
    iface.launch()